{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9o2wjC2I2YmYBGwTmpKiC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RudyMartin/esp32-ai-agents/blob/main/camp_navigator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAB 1 Demo"
      ],
      "metadata": {
        "id": "bTaDL7rrOFS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 1Ô∏è‚É£  Install required libraries\n",
        "# ---------------------------------------------------------------------\n",
        "!pip install --quiet openai scikit-learn requests\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2Ô∏è‚É£  Configure API keys\n",
        "# ---------------------------------------------------------------------\n",
        "import os, getpass, requests, textwrap, json\n",
        "from urllib.parse import quote_plus\n",
        "from openai import OpenAI          # NEW import\n",
        "\n",
        "# OpenAI key ‚Äì never commit this to GitHub!\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or getpass.getpass(\"üîë Paste your OpenAI key ‚Üí \")\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)   # NEW client object\n",
        "\n",
        "# Optional: GitHub token to dodge rate-limits (60 req/hr unauth)\n",
        "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\") or \"\"\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3Ô∏è‚É£  Recursively download ALL text files from the repo\n",
        "# ---------------------------------------------------------------------\n",
        "REPO   = \"RudyMartin/dsai-2025\"\n",
        "BRANCH = \"main\"\n",
        "API_URL = f\"https://api.github.com/repos/{REPO}/git/trees/{BRANCH}?recursive=1\"\n",
        "\n",
        "headers = {\"Accept\": \"application/vnd.github+json\"}\n",
        "if GITHUB_TOKEN:\n",
        "    headers[\"Authorization\"] = f\"token {GITHUB_TOKEN}\"\n",
        "\n",
        "def fetch_tree(url):\n",
        "    \"\"\"Return the JSON tree (handles pagination for very large repos).\"\"\"\n",
        "    r = requests.get(url, headers=headers)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    yield from data.get(\"tree\", [])\n",
        "    if data.get(\"truncated\"):\n",
        "        yield from fetch_tree(data[\"url\"])\n",
        "\n",
        "EXT_OK = (\".md\", \".txt\", \".ino\", \".py\", \".csv\", \".json\")\n",
        "file_paths = [\n",
        "    item[\"path\"]\n",
        "    for item in fetch_tree(API_URL)\n",
        "    if item[\"type\"] == \"blob\" and item[\"path\"].lower().endswith(EXT_OK)\n",
        "]\n",
        "\n",
        "print(f\"üìÇ Found {len(file_paths)} files‚Ä¶ downloading\")\n",
        "\n",
        "RAW_BASE = f\"https://raw.githubusercontent.com/{REPO}/{BRANCH}/\"\n",
        "docs, loaded = [], 0\n",
        "for path in file_paths:\n",
        "    r = requests.get(RAW_BASE + quote_plus(path), headers=headers)\n",
        "    if r.status_code == 200:\n",
        "        docs.append(r.text)\n",
        "        loaded += 1\n",
        "        if loaded % 25 == 0:\n",
        "            print(f\"  ‚Ä¶{loaded} files\")\n",
        "    else:\n",
        "        print(f\"‚ùå {path} ({r.status_code}) skipped\")\n",
        "\n",
        "full_text = \"\\n\\n\".join(docs)\n",
        "print(f\"‚úÖ Loaded {loaded} files ‚Ä¢ {len(full_text)//1000} KB of text\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4Ô∏è‚É£  Chunk text & build TF-IDF index\n",
        "# ---------------------------------------------------------------------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "chunks = textwrap.wrap(full_text, width=1000, break_long_words=False)\n",
        "vectorizer   = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "\n",
        "print(f\"üîç Indexed {len(chunks)} chunks\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5Ô∏è‚É£  Camp Navigator interactive loop\n",
        "# ---------------------------------------------------------------------\n",
        "def navigator_chat(k_top: int = 3, temperature: float = 0.3):\n",
        "    \"\"\"\n",
        "    Simple CLI chatbot.  Type 'quit' to exit.\n",
        "    \"\"\"\n",
        "    print(\"\\nüöÄ Camp Navigator ready ‚Äì ask me anything about the camp (type quit to exit)\\n\")\n",
        "    while True:\n",
        "        query = input(\"‚ùì Your question ‚Üí \").strip()\n",
        "        if query.lower() in {\"quit\", \"exit\"}:\n",
        "            break\n",
        "\n",
        "        # SEE & THINK\n",
        "        q_vec   = vectorizer.transform([query])\n",
        "        scores  = cosine_similarity(q_vec, tfidf_matrix).flatten()\n",
        "        top_idxs = scores.argsort()[-k_top:][::-1]\n",
        "        context  = \"\\n\\n\".join(chunks[i] for i in top_idxs)[:4000]   # safety cap\n",
        "\n",
        "        # DO\n",
        "        prompt = (\n",
        "            \"You are Camp Navigator, an AI assistant for the Artemis DSAI 2025 camp.\\n\"\n",
        "            \"Use the camp information below to answer the student clearly and concisely.\\n\\n\"\n",
        "            f\"{context}\\n\\n\"\n",
        "            f\"Q: {query}\\nA:\"\n",
        "        )\n",
        "\n",
        "        resp = client.chat.completions.create(          # NEW call style\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            temperature=temperature,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        )\n",
        "\n",
        "        # SPEAK\n",
        "        print(\"\\nüß† ùôàùòæùôã Trace: See ‚Üí Think ‚Üí Do ‚Üí Speak\")\n",
        "        print(\"üîç Top-match snippets shown to GPT (truncated):\")\n",
        "        for idx in top_idxs:\n",
        "            snippet = chunks[idx][:150].replace(\"\\n\", \" \")\n",
        "            print(f\"  ‚Ä¢ ‚Ä¶{snippet}‚Ä¶\")\n",
        "        print(\"\\nü§ñ Answer:\", resp.choices[0].message.content, \"\\n\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6Ô∏è‚É£  Go!\n",
        "# ---------------------------------------------------------------------\n",
        "navigator_chat()\n"
      ],
      "metadata": {
        "id": "sKMOdDKbOLLw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
